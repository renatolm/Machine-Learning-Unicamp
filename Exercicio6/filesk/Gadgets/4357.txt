

Popular Science was one of my favorite magazines as a teenager. I was amazed by the inventions of mere mortals, and wondered if there was some sort of genetic mutation that allowed a select few to see the future more clearly than the rest of us. Secretly, I hoped to be a visionary someday.
I’ve been blessed to work side-by-side with a couple of visionaries in my career. Ken Williams, founder of Sierra Online, created the first graphical PC game, Mystery House, in 1979. George Lucas not only created Star Wars, but also what is now Pixar, THX, AVID and many more. Both are widely recognized as visionaries.
Both men are wicked smart and naturally curious, two characteristics often attributed to visionaries. What people often miss, however, is that they are both brilliant businessmen. They understand you can have too much vision, seeing a far-off future that won’t come true within the right time frame. As an entrepreneur, the challenge is funding from where you are today until enough customers agree with your vision that they pay for your expenses. You need to know where you are on the vision timeline.
Ken Williams decided to sell Sierra Online to CUC International in 1996. By 1997, neither of us could stand our new boss, so we quit. We spent the summer of 1997 brainstorming ideas for a new company and finally settled on Digital Storage, Inc. The basic idea was to convert all your major life moments to digital and preserve them forever, even allowing you to assign digital heirs.
We both were 100 percent convinced it would be a huge business someday, but we were concerned we might be too early. Rather than spend millions of dollars building the service, we decided to spend about $50,000 on focus groups, cold-calling potential customers and a half-page color ad in USA Today advertising our service. After three months of effort only one person signed up. Our vision may have been right, but our timing was off — we were way too early.
Fast-forward to 2008 when I started working with George Lucas and Lucasfilm as a board member and advisor. George carried a laminated card in his wallet of ideas he had for future products. One of those ideas was G-Whiz.
A challenge all directors face is going from 2D storyboards to shooting a scene on an unfamiliar location selected by a scout. Figuring out in real time where to place the cameras, actors, mics, props and crew when the location didn’t match the storyboard was time-consuming and expensive. He knew ILM had a huge library of props and characters and could generate a life-like model of a set, but he couldn’t explore it in real time.
While visiting LucasArts and watching them build levels in real time on a PlayStation 2, George started thinking about how to merge the game-builder’s ability to create a level in real time with the director’s need to create a set immediately. He also was watching the progress Lucasfilm R&D (now Lucas Labs) was making with real-time compositing. Live actors could walk around a completely empty green screen set while the director could see them in real time on the CG set, in costume with a chunk of 2×4 transformed into a blaster.
George acutely understood the challenges facing directors, but he had access to technology and research that most people never would see. I believe that enabled him to crystallize a vision for G-Whiz that would have taken anyone else years longer to develop. He also had the patience (and money) to slowly bring his vision to life as the movie industry transitioned from analog film to digital. The vision was important, but so was the timeline.
So why write an article about vision for a tech blog? I believe we are farther along the timeline for apps based on open standards than most visionaries are predicting today. Wait! Before you quit reading and jump straight to the comments to explain to me in technical terms why I’m wrong, please understand I’m going to present a business case for open technologies.
From a business perspective, I’ve seen Act I of this play during the flip-phone era. Back then, I was COO in charge of development at EA Canada; one of our most popular games was FIFA. In 2003, we released it for mobile phone at a cost of X. Over the next three years, as flip-phone sales exploded, we had to support Java, Brew, Symbian, Blackberry and Windows Mobile. Our cost to develop FIFA on mobile was nearly 2X, but revenue growth was much less. The demand for people who wanted to play FIFA on their phone was relatively static, but they wanted to play it on their favorite phone. That was our (very expensive) problem to solve.
Today we have smartphones, tablets, wearables, TVs and Internet of Things to support. It is simply not profitable for brands and IT departments to create native app versions for all these devices running a plethora of operating systems (and don’t even get me started about Android being consistently lumped together as if it were a single OS to support).
Ken Williams and George Lucas could see things others couldn’t yet because of their vantage points in their industries. Who has that perspective today? I’d argue that it’s the people who are in corporate IT management roles who can see that the proliferation of browsers and devices is moving us toward a breaking point for the native app-centric model.
The extreme popularity and growth of smart devices over the past seven years, along with a growing economy, has masked the underlying business problem. Corporate IT departments were the first to realize this, and have aggressively been engaging “create once, run anywhere” solutions from Kony, Sencha and others.
Corporations spend twice as much money ($40 billion) on “in-house” apps as consumers do in the Apple, Google and Microsoft stores combined, and they don’t care or need distribution by Google, Apple or Microsoft’s store. They have a captive audience and they know how to reach every single one of them. They are embracing open standards, including HTML5. When that isn’t feasible, such as needing direct access to a hardware sensor, they use hybrid apps where the native app functionality is scarcely more than a wrapper. Developing native apps for a half-dozen different operating systems is a last resort, and one that increasingly doesn’t work. It’s time for an alternative.
What’s the alternative? Ask those with the industry vantage point and they will tell you the native app model is already starting to fail. They’ll point to a future where something like “on-demand apps,” as I call them, work on every device and can be centrally managed by the corporation without going through a proprietary app store. This will seem unrealistic to some, just as walking around a virtual set in real time seemed impossible to most a decade ago. They were right — at the time. And then, suddenly, they were wrong. I believe we’ll see the same thing happen with corporate apps (and then many consumer-facing apps).
Visionaries benefit from perspective and experiences not readily accessible to most of us. Sometimes, to be a visionary, you need to find a vantage point that allows you to see just a little further than everyone else. From where I stand — with a history in creating content for an ever-increasing variety of platforms — I believe the shift to on-demand content that works across all browsers is happening right now.
